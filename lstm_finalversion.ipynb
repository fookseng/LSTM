{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fs_lstm_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fookseng/LSTM/blob/main/lstm_finalversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4PYRMwNAI3H"
      },
      "source": [
        "This code is to predict the pixel value at time T, provided data on time (T-2) and time(T-1). And is able to run the following cases:\n",
        "\n",
        "\n",
        "*   Univariate stateless lstm (feature = pixel) (mode 0)\n",
        "*   Univariate stateful lstm (feature = pixel) (mode 0)\n",
        "\n",
        "*   Multivariate stateless lstm\n",
        "*   Multivariate stateful lstm\n",
        "> For multivariate LSTM model features, we provide the following combinations.\n",
        " - features = pixel + pixels (mode 1)\n",
        " - features = pixel + pixels + flow (mode 2)\n",
        " - features = pixel + pixels + tide (mode 3)\n",
        " - features = pixel + pixels + flow + tide (mode 4)\n",
        " - features = pixel + flow (mode 5)\n",
        " - features = pixel + tide (mode 6)\n",
        " - features = pixel + flow + tide (mode 7)\n",
        " \n",
        " * \"pixels\"(o) are the pixels surrounding the \"pixel\"(*). \n",
        " -ooo\n",
        " -o*o\n",
        " -ooo\n",
        "\n",
        "**How to use this code?**\n",
        "*You only need to **make 4 modifications** in the \"Variables initialization\" zone.*\n",
        "1. Select \"stateful\" or \"stateless\" by modifying the variable \"stateful\".\n",
        "2. Select \"TSS\" or \"CHL\" by modifying the variable \"tss\".\n",
        "3. Choose the (x, y) coordinate you wish to train model. Modify the variables \"x_pos\" and \"y_pos\".\n",
        "4. Select a mode by modifying the variable \"mode\"(0 to 7).\n",
        "5. You may modify other variables to meet your own needs. But the above modifications are sufficient to run this code.\n",
        "\n",
        "**Extra Note:**\n",
        "*To train multiple models. Simply apply a for loop on the main function. Remember to re-initialize all the variables for each loop, as colab will not clear the variables unless you restart the runtime. Therefore, the previous values will affect the current model. *\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Paixq4oLRyQa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEtz6i4PHaWs"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow # for image display\n",
        "from tensorflow.keras.models import Sequential, model_from_json, load_model\n",
        "from tensorflow.keras.layers import InputLayer, Dense, Activation, Flatten, LeakyReLU, ReLU, PReLU, Dropout, BatchNormalization, LSTM\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Nadam, Adadelta \n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from PIL import Image as im\n",
        "from keras.preprocessing.image import image\n",
        "import shutil\n",
        "import imageio\n",
        "from numpy import array\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTXh5UMpHj6T",
        "outputId": "aa8c2537-3634-47ad-db6c-a8de35de3f92"
      },
      "source": [
        "# Check tensorflow version\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MldnYJFLHl6M",
        "outputId": "89d0480f-c1fb-497f-efe4-8164d3b914fb"
      },
      "source": [
        "# Mount Google Drive to access data\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nihv6it-Htgo"
      },
      "source": [
        "# *Variables initialization*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahW_MWKYHxys"
      },
      "source": [
        "# The data path for \"continuous_for_3_yearly\".\n",
        "path = '/content/gdrive/Shareddrives/namr - water quality forecast/1005data_OutletOnly(8x12)/CHL/continuous_for_3_yearly'\n",
        "# The path for saving model\n",
        "savePath = '/content/gdrive/Shareddrives/namr - water quality forecast/fs/result/'\n",
        "\n",
        "### Define model type\n",
        "# Stateful or Stateless. If Stateful, set it to \"True\", else \"False\"\n",
        "stateful = True\n",
        "state_name = 'stateful' if stateful else 'stateless'\n",
        "# TSS or CHL model? Set the variable 'tss' to True if this is a TSS model, else false.\n",
        "tss = False\n",
        "model_name = \"tss\" if tss else 'chl'\n",
        "# The coordinate of pixel\n",
        "x_pos, y_pos = 2, 2\n",
        "# Mode selection\n",
        "mode = 3\n",
        "# Univariate or Multivariate model (True=Univariate, False=Multivariate)\n",
        "univariate = False if mode else True\n",
        "model_type = \"univariate\" if univariate else 'multivariate'\n",
        "# model save path\n",
        "filepath = savePath +'/model_'+ model_type+ '_'+ state_name+ '_'+ \"mode\"+ str(mode)+ '_'+ str(x_pos)+ '_'+ str(y_pos)+ '_' +model_name+'.h5'\n",
        "\n",
        "### Model parameters\n",
        "# number of time steps\n",
        "n_steps = 2\n",
        "# Learning Rate\n",
        "learningRate = 0.001\n",
        "# Batch size. For stateful LSTM, batch size must be set properly. Change the batch size only if you sure what you are doing.\n",
        "n_batch = 6\n",
        "# Number of epoch.\n",
        "n_epoch = 100\n",
        "# number of features\n",
        "if (mode == 0):\n",
        "  n_features = 1\n",
        "elif (mode == 1):\n",
        "  n_features = 9\n",
        "elif (mode == 2):\n",
        "  n_features = 10\n",
        "elif (mode == 3):\n",
        "  n_features = 10\n",
        "elif (mode == 4):\n",
        "  n_features = 11\n",
        "elif (mode == 5):\n",
        "  n_features = 2\n",
        "elif (mode == 6):\n",
        "  n_features = 2\n",
        "else:\n",
        "  n_features = 3\n",
        "\n",
        "### Define some variables\n",
        "# Define some variables.\n",
        "pixel_data_train = []\n",
        "flow_data_train = []\n",
        "tide_data_train = []\n",
        "pixel1_data_train =[]\n",
        "pixel2_data_train =[]\n",
        "pixel3_data_train =[]\n",
        "pixel4_data_train =[]\n",
        "pixel5_data_train =[]\n",
        "pixel6_data_train =[]\n",
        "pixel7_data_train =[]\n",
        "pixel8_data_train =[]\n",
        "\n",
        "pixel_data_test = []\n",
        "flow_data_test = []\n",
        "tide_data_test = []\n",
        "pixel1_data_test = []\n",
        "pixel2_data_test = []\n",
        "pixel3_data_test = []\n",
        "pixel4_data_test = []\n",
        "pixel5_data_test = []\n",
        "pixel6_data_test = []\n",
        "pixel7_data_test = []\n",
        "pixel8_data_test = []"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxZdJe9KtjV"
      },
      "source": [
        "# *Functions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DxLG5dkKwGt"
      },
      "source": [
        "def create_data(x, y):\n",
        "  '''\n",
        "  We will combine dataset from 2011-2017 and 2019 to use as train and valid dataset (8:2).\n",
        "  2018 dataset will be  used as test set.\n",
        "  > T2 is time(T-2), T1 is time(T-1), and T is current time T.\n",
        "  '''\n",
        "  # Pixel dataset\n",
        "  images_T2_2019 = np.load(path +'/image_array_0_2019.npy')\n",
        "  images_T2_2018 = np.load(path +'/image_array_0_2018.npy')\n",
        "  images_T2_2011_2017 = np.load(path +'/image_array_0_2011_2017.npy')\n",
        "  images_T1_2019 = np.load(path +'/image_array_1_2019.npy')\n",
        "  images_T1_2018 = np.load(path +'/image_array_1_2018.npy')\n",
        "  images_T1_2011_2017 = np.load(path +'/image_array_1_2011_2017.npy')\n",
        "  images_T_2019 = np.load(path +'/image_array_2_2019.npy')\n",
        "  images_T_2018 = np.load(path +'/image_array_2_2018.npy')\n",
        "  images_T_2011_2017 = np.load(path +'/image_array_2_2011_2017.npy')\n",
        "  # Flow dataset\n",
        "  flow_T2_2019 = np.load(path +'/flow_array_0_2019.npy')\n",
        "  flow_T2_2018 = np.load(path +'/flow_array_0_2018.npy')\n",
        "  flow_T2_2011_2017 = np.load(path +'/flow_array_0_2011_2017.npy')\n",
        "  flow_T1_2019 = np.load(path +'/flow_array_1_2019.npy')\n",
        "  flow_T1_2018 = np.load(path +'/flow_array_1_2018.npy')\n",
        "  flow_T1_2011_2017 = np.load(path +'/flow_array_1_2011_2017.npy')\n",
        "  flow_T_2019 = np.load(path +'/flow_array_2_2019.npy')\n",
        "  flow_T_2018 = np.load(path +'/flow_array_2_2018.npy')\n",
        "  flow_T_2011_2017 = np.load(path +'/flow_array_2_2011_2017.npy')\n",
        "  # Tide dataset\n",
        "  tide_T2_2019 = np.load(path +'/delta_tide_array_0_2019.npy')\n",
        "  tide_T2_2018 = np.load(path +'/delta_tide_array_0_2018.npy')\n",
        "  tide_T2_2011_2017 = np.load(path +'/delta_tide_array_0_2011_2017.npy')\n",
        "  tide_T1_2019 = np.load(path +'/delta_tide_array_1_2019.npy')\n",
        "  tide_T1_2018 = np.load(path +'/delta_tide_array_1_2018.npy')\n",
        "  tide_T1_2011_2017 = np.load(path +'/delta_tide_array_1_2011_2017.npy')\n",
        "  tide_T_2019 = np.load(path +'/delta_tide_array_2_2019.npy')\n",
        "  tide_T_2018 = np.load(path +'/delta_tide_array_2_2018.npy')\n",
        "  tide_T_2011_2017 = np.load(path +'/delta_tide_array_2_2011_2017.npy')\n",
        "\n",
        "  print(\"Creating dataset on point:\", x,\",\", y)\n",
        "  for i in range(tide_T_2011_2017.size):\n",
        "      pixel_data_train.append(images_T2_2011_2017[i][y][x])\n",
        "      pixel_data_train.append(images_T1_2011_2017[i][y][x])\n",
        "      pixel_data_train.append(images_T_2011_2017[i][y][x])\n",
        "      flow_data_train.append(flow_T2_2011_2017[i][0])\n",
        "      flow_data_train.append(flow_T1_2011_2017[i][0])\n",
        "      flow_data_train.append(flow_T_2011_2017[i][0])\n",
        "      tide_data_train.append(tide_T2_2011_2017[i][0])\n",
        "      tide_data_train.append(tide_T1_2011_2017[i][0])\n",
        "      tide_data_train.append(tide_T_2011_2017[i][0])\n",
        "  for i in range(tide_T_2019.size):\n",
        "      pixel_data_train.append(images_T2_2019[i][y][x])\n",
        "      pixel_data_train.append(images_T1_2019[i][y][x])\n",
        "      pixel_data_train.append(images_T_2019[i][y][x])\n",
        "      flow_data_train.append(flow_T2_2019[i][0])\n",
        "      flow_data_train.append(flow_T1_2019[i][0])\n",
        "      flow_data_train.append(flow_T_2019[i][0])\n",
        "      tide_data_train.append(tide_T2_2019[i][0])\n",
        "      tide_data_train.append(tide_T1_2019[i][0])\n",
        "      tide_data_train.append(tide_T_2019[i][0])\n",
        "  for i in range(tide_T_2018.size):\n",
        "      pixel_data_test.append(images_T2_2018[i][y][x])\n",
        "      pixel_data_test.append(images_T1_2018[i][y][x])\n",
        "      pixel_data_test.append(images_T_2018[i][y][x])\n",
        "      flow_data_test.append(flow_T2_2018[i][0])\n",
        "      flow_data_test.append(flow_T1_2018[i][0])\n",
        "      flow_data_test.append(flow_T_2018[i][0])\n",
        "      tide_data_test.append(tide_T2_2018[i][0])\n",
        "      tide_data_test.append(tide_T1_2018[i][0])\n",
        "      tide_data_test.append(tide_T_2018[i][0])\n",
        "  \n",
        "  if(mode >= 1 and mode <= 4):\n",
        "    for i in range(tide_T_2011_2017.size):\n",
        "      pixel1_data_train.append(images_T2_2011_2017[i][y-1][x+1])\n",
        "      pixel1_data_train.append(images_T1_2011_2017[i][y-1][x+1])\n",
        "      pixel1_data_train.append(images_T_2011_2017[i][y-1][x+1])\n",
        "      pixel2_data_train.append(images_T2_2011_2017[i][y][x+1])\n",
        "      pixel2_data_train.append(images_T1_2011_2017[i][y][x+1])\n",
        "      pixel2_data_train.append(images_T_2011_2017[i][y][x+1])\n",
        "      pixel3_data_train.append(images_T2_2011_2017[i][y+1][x+1])\n",
        "      pixel3_data_train.append(images_T1_2011_2017[i][y+1][x+1])\n",
        "      pixel3_data_train.append(images_T_2011_2017[i][y+1][x+1])\n",
        "      pixel4_data_train.append(images_T2_2011_2017[i][y-1][x])\n",
        "      pixel4_data_train.append(images_T1_2011_2017[i][y-1][x])\n",
        "      pixel4_data_train.append(images_T_2011_2017[i][y-1][x])\n",
        "      pixel5_data_train.append(images_T2_2011_2017[i][y+1][x])\n",
        "      pixel5_data_train.append(images_T1_2011_2017[i][y+1][x])\n",
        "      pixel5_data_train.append(images_T_2011_2017[i][y+1][x])\n",
        "      pixel6_data_train.append(images_T2_2011_2017[i][y-1][x-1])\n",
        "      pixel6_data_train.append(images_T1_2011_2017[i][y-1][x-1])\n",
        "      pixel6_data_train.append(images_T_2011_2017[i][y-1][x-1])\n",
        "      pixel7_data_train.append(images_T2_2011_2017[i][y][x-1])\n",
        "      pixel7_data_train.append(images_T1_2011_2017[i][y][x-1])\n",
        "      pixel7_data_train.append(images_T_2011_2017[i][y][x-1])\n",
        "      pixel8_data_train.append(images_T2_2011_2017[i][y+1][x-1])\n",
        "      pixel8_data_train.append(images_T1_2011_2017[i][y+1][x-1])\n",
        "      pixel8_data_train.append(images_T_2011_2017[i][y+1][x-1])\n",
        "    for i in range(tide_T_2019.size):\n",
        "      pixel1_data_train.append(images_T2_2019[i][y-1][x+1])\n",
        "      pixel1_data_train.append(images_T1_2019[i][y-1][x+1])\n",
        "      pixel1_data_train.append(images_T_2019[i][y-1][x+1])\n",
        "      pixel2_data_train.append(images_T2_2019[i][y][x+1])\n",
        "      pixel2_data_train.append(images_T1_2019[i][y][x+1])\n",
        "      pixel2_data_train.append(images_T_2019[i][y][x+1])\n",
        "      pixel3_data_train.append(images_T2_2019[i][y+1][x+1])\n",
        "      pixel3_data_train.append(images_T1_2019[i][y+1][x+1])\n",
        "      pixel3_data_train.append(images_T_2019[i][y+1][x+1])\n",
        "      pixel4_data_train.append(images_T2_2019[i][y-1][x])\n",
        "      pixel4_data_train.append(images_T1_2019[i][y-1][x])\n",
        "      pixel4_data_train.append(images_T_2019[i][y-1][x])\n",
        "      pixel5_data_train.append(images_T2_2019[i][y+1][x])\n",
        "      pixel5_data_train.append(images_T1_2019[i][y+1][x])\n",
        "      pixel5_data_train.append(images_T_2019[i][y+1][x])\n",
        "      pixel6_data_train.append(images_T2_2019[i][y-1][x-1])\n",
        "      pixel6_data_train.append(images_T1_2019[i][y-1][x-1])\n",
        "      pixel6_data_train.append(images_T_2019[i][y-1][x-1])\n",
        "      pixel7_data_train.append(images_T2_2019[i][y][x-1])\n",
        "      pixel7_data_train.append(images_T1_2019[i][y][x-1])\n",
        "      pixel7_data_train.append(images_T_2019[i][y][x-1])\n",
        "      pixel8_data_train.append(images_T2_2019[i][y+1][x-1])\n",
        "      pixel8_data_train.append(images_T1_2019[i][y+1][x-1])\n",
        "      pixel8_data_train.append(images_T_2019[i][y+1][x-1])\n",
        "    for i in range(tide_T_2018.size):\n",
        "      pixel1_data_test.append(images_T2_2018[i][y-1][x+1])\n",
        "      pixel1_data_test.append(images_T1_2018[i][y-1][x+1])\n",
        "      pixel1_data_test.append(images_T_2018[i][y-1][x+1])\n",
        "      pixel2_data_test.append(images_T2_2018[i][y][x+1])\n",
        "      pixel2_data_test.append(images_T1_2018[i][y][x+1])\n",
        "      pixel2_data_test.append(images_T_2018[i][y][x+1])\n",
        "      pixel3_data_test.append(images_T2_2018[i][y+1][x+1])\n",
        "      pixel3_data_test.append(images_T1_2018[i][y+1][x+1])\n",
        "      pixel3_data_test.append(images_T_2018[i][y+1][x+1])\n",
        "      pixel4_data_test.append(images_T2_2018[i][y-1][x])\n",
        "      pixel4_data_test.append(images_T1_2018[i][y-1][x])\n",
        "      pixel4_data_test.append(images_T_2018[i][y-1][x])\n",
        "      pixel5_data_test.append(images_T2_2018[i][y+1][x])\n",
        "      pixel5_data_test.append(images_T1_2018[i][y+1][x])\n",
        "      pixel5_data_test.append(images_T_2018[i][y+1][x])\n",
        "      pixel6_data_test.append(images_T2_2018[i][y-1][x-1])\n",
        "      pixel6_data_test.append(images_T1_2018[i][y-1][x-1])\n",
        "      pixel6_data_test.append(images_T_2018[i][y-1][x-1])\n",
        "      pixel7_data_test.append(images_T2_2018[i][y][x-1])\n",
        "      pixel7_data_test.append(images_T1_2018[i][y][x-1])\n",
        "      pixel7_data_test.append(images_T_2018[i][y][x-1])\n",
        "      pixel8_data_test.append(images_T2_2018[i][y+1][x-1])\n",
        "      pixel8_data_test.append(images_T1_2018[i][y+1][x-1])\n",
        "      pixel8_data_test.append(images_T_2018[i][y+1][x-1])\n",
        "        \n",
        "\n",
        "  # print(images_T2_2011_2017.size, images_T2_2011_2017.shape)\n",
        "  # print(images_T2_2019.size, images_T2_2019.shape)\n",
        "  # print(images_T2_2018.size, images_T2_2018.shape)\n",
        "  # print(images_T1_2011_2017.size, images_T1_2011_2017.shape)\n",
        "  # print(images_T1_2019.size, images_T1_2019.shape)\n",
        "  # print(images_T1_2018.size, images_T1_2018.shape)\n",
        "  # print(images_T_2011_2017.size, images_T_2011_2017.shape)\n",
        "  # print(images_T_2019.size, images_T_2019.shape)\n",
        "  # print(images_T_2018.size, images_T_2018.shape)\n",
        "  # print(len(pixel_data_train), len(pixel_data_test))\n",
        "  # print(\"\\n\\n\")\n",
        "  # print(tide_T2_2011_2017.size, tide_T2_2011_2017.shape)\n",
        "  # print(tide_T2_2019.size, tide_T2_2019.shape)\n",
        "  # print(tide_T2_2018.size, tide_T2_2018.shape)\n",
        "  # print(tide_T1_2011_2017.size, tide_T1_2011_2017.shape)\n",
        "  # print(tide_T1_2019.size, tide_T1_2019.shape)\n",
        "  # print(tide_T1_2018.size, tide_T1_2018.shape)\n",
        "  # print(tide_T_2011_2017.size, tide_T_2011_2017.shape)\n",
        "  # print(tide_T_2019.size, tide_T_2019.shape)\n",
        "  # print(tide_T_2018.size, tide_T_2018.shape)\n",
        "  # print(len(tide_data_train), len(tide_data_test))\n",
        "  # print(\"\\n\\n\")\n",
        "  return tide_T_2019.size+tide_T_2011_2017.size"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK2cV5yPSlmf"
      },
      "source": [
        "# design network\n",
        "def build_model_stateful(n_features):\n",
        "    model = Sequential()\n",
        "    #model.add(LSTM(64, input_shape=(n_steps, n_features), return_sequences= True, batch_input_shape=(n_batch, x_sampled.shape[1], x_sampled.shape[2]), stateful=True))\n",
        "    model.add(LSTM(64, input_shape=(n_steps, n_features), return_sequences= True, batch_input_shape=(1, x_sampled.shape[1], x_sampled.shape[2]), stateful=True))\n",
        "    model.add(LSTM(32, input_shape=(n_steps, n_features), return_sequences=False, batch_input_shape=(1, x_sampled.shape[1], x_sampled.shape[2]), stateful=True))\n",
        "    model.add(Dense(10, activation=ReLU()))\n",
        "    model.add(Dense(1, activation=ReLU()))\n",
        "    model.compile(loss='mean_squared_error', optimizer= Nadam(learning_rate=learningRate), metrics=['mean_squared_error'])\n",
        "    return model\n",
        "def build_model_stateless(n_features):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=(n_steps, n_features), return_sequences= True))\n",
        "    model.add(LSTM(32, return_sequences=False))\n",
        "    model.add(Dense(10, activation=ReLU()))\n",
        "    model.add(Dense(1, activation=ReLU()))\n",
        "    model.compile(loss='mean_squared_error', optimizer= Nadam(learning_rate=learningRate), metrics=['mean_squared_error'])\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vayj1hg6Sphd"
      },
      "source": [
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(0, len(sequence), 3):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)\n",
        "def multi_data_prep(dataset, target):\n",
        "     X = []\n",
        "     y = []\n",
        "     for i in range(0, len(dataset), 3):\n",
        "         X.append(dataset[i:i+2])\n",
        "         y.append(target[i+2])\n",
        "     return np.array(X), np.array(y) "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIn7RmkmTDwK"
      },
      "source": [
        "def reset_variables():\n",
        "  pixel_data_train = []\n",
        "  flow_data_train = []\n",
        "  tide_data_train = []\n",
        "  pixel1_data_train =[]\n",
        "  pixel2_data_train =[]\n",
        "  pixel3_data_train =[]\n",
        "  pixel4_data_train =[]\n",
        "  pixel5_data_train =[]\n",
        "  pixel6_data_train =[]\n",
        "  pixel7_data_train =[]\n",
        "  pixel8_data_train =[]\n",
        "\n",
        "  pixel_data_test = []\n",
        "  flow_data_test = []\n",
        "  tide_data_test = []\n",
        "  pixel1_data_test = []\n",
        "  pixel2_data_test = []\n",
        "  pixel3_data_test = []\n",
        "  pixel4_data_test = []\n",
        "  pixel5_data_test = []\n",
        "  pixel6_data_test = []\n",
        "  pixel7_data_test = []\n",
        "  pixel8_data_test = []\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J4l8SxATFGb"
      },
      "source": [
        "# *Main function*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zArOFvnTIrV",
        "outputId": "f768cbaf-87e8-4a5d-e905-419dd49aaf4f"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  ### Create data and save as pandas DataFrame\n",
        "  m = create_data(x_pos, y_pos)\n",
        "  print(\"m: \" + str(m))\n",
        "  if (mode >= 1 and mode <= 4):\n",
        "    dic_train = {'pixel': pixel_data_train, 'pixel1': pixel1_data_train, 'pixel2': pixel2_data_train, 'pixel3': pixel3_data_train, 'pixel4': pixel4_data_train, 'pixel5': pixel5_data_train, 'pixel6': pixel6_data_train, 'pixel7': pixel7_data_train, 'pixel8': pixel8_data_train, 'flow': flow_data_train, 'tide': tide_data_train}\n",
        "    dic_test = {'pixel': pixel_data_test, 'pixel1': pixel1_data_test, 'pixel2': pixel2_data_test, 'pixel3': pixel3_data_test, 'pixel4': pixel4_data_test, 'pixel5': pixel5_data_test, 'pixel6': pixel6_data_test, 'pixel7': pixel7_data_test, 'pixel8': pixel8_data_test, 'flow': flow_data_test, 'tide': tide_data_test}\n",
        "  else:\n",
        "    dic_train = {'pixel': pixel_data_train, 'flow': flow_data_train, 'tide': tide_data_train}\n",
        "    dic_test = {'pixel': pixel_data_test, 'flow': flow_data_test, 'tide': tide_data_test}\n",
        "  df_train = pd.DataFrame(dic_train)\n",
        "  df_test = pd.DataFrame(dic_test)\n",
        "  \n",
        "  '''\n",
        "  ### Observe the statistics and frequency of dataset.\n",
        "  # df.describe().transpose()\n",
        "  plot_cols = ['pixel', 'flow', 'tide']\n",
        "  plot_features = df_test[plot_cols]\n",
        "  _ = plot_features.plot(subplots=True)\n",
        "  plot_features = df_test[plot_cols][:80]\n",
        "  _ = plot_features.plot(subplots=True)\n",
        "\n",
        "  ### Perform MinMax Scaling\n",
        "  # Scale variables (tide and flow)\n",
        "  features_scaler = MinMaxScaler()\n",
        "  features_data = features_scaler.fit_transform(df[[\"flow\", \"tide\"]])\n",
        "  # Scale target (pixel)\n",
        "  target_scaler = MinMaxScaler()\n",
        "  target_data = target_scaler.fit_transform(df[[\"pixel\"]])\n",
        "  '''\n",
        "  features_scaler = MinMaxScaler()\n",
        "  target_scaler = MinMaxScaler()\n",
        "\n",
        "  ### Data Preprocessing.\n",
        "  if (univariate):\n",
        "    ### split into samples. These samples will be fed into network for training.\n",
        "    x_sampled, y_sampled = split_sequence(pixel_data_train, n_steps)\n",
        "    x_test, y_test = split_sequence(pixel_data_test, n_steps)\n",
        "  else:\n",
        "    if (mode == 1):\n",
        "      features_data = features_scaler.fit_transform(df_train[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\"]])\n",
        "      features_data_test = features_scaler.fit_transform(df_test[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\"]])\n",
        "      #features_data = df_train[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\"]].to_numpy()\n",
        "      #features_data_test = df_test[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\"]].to_numpy()\n",
        "    elif (mode == 2):\n",
        "      features_data = features_scaler.fit_transform(df_train[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"flow\"]])\n",
        "      features_data_test = features_scaler.fit_transform(df_test[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"flow\"]])\n",
        "      #features_data = df_train[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"flow\"]].to_numpy()\n",
        "      #features_data_test = df_test[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"flow\"]].to_numpy()\n",
        "    elif (mode == 3):\n",
        "      features_data = features_scaler.fit_transform(df_train[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"tide\"]])\n",
        "      features_data_test = features_scaler.fit_transform(df_test[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"tide\"]])\n",
        "      #features_data = df_train[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"tide\"]].to_numpy()\n",
        "      #features_data_test = df_test[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"tide\"]].to_numpy()\n",
        "    elif (mode == 4):\n",
        "      features_data = features_scaler.fit_transform(df_train[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"flow\", \"tide\"]])\n",
        "      features_data_test = features_scaler.fit_transform(df_test[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"flow\", \"tide\"]])\n",
        "      #features_data = df_train[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"flow\", \"tide\"]].to_numpy()\n",
        "      #features_data_test = df_test[[\"pixel\", \"pixel1\", \"pixel2\", \"pixel3\", \"pixel4\", \"pixel5\", \"pixel6\", \"pixel7\", \"pixel8\", \"flow\", \"tide\"]].to_numpy()\n",
        "    elif (mode == 5):\n",
        "      features_data = features_scaler.fit_transform(df_train[[\"pixel\", \"flow\"]])\n",
        "      features_data_test = features_scaler.fit_transform(df_test[[\"pixel\", \"flow\"]])\n",
        "      #features_data = df_train[[\"pixel\", \"flow\"]].to_numpy()\n",
        "      #features_data_test = df_test[[\"pixel\", \"flow\"]].to_numpy()\n",
        "    elif (mode == 6):\n",
        "      features_data = features_scaler.fit_transform(df_train[[\"pixel\", \"tide\"]])\n",
        "      features_data_test = features_scaler.fit_transform(df_test[[\"pixel\", \"tide\"]])\n",
        "      #features_data = df_train[[\"pixel\", \"tide\"]].to_numpy()\n",
        "      #features_data_test = df_test[[\"pixel\", \"tide\"]].to_numpy()\n",
        "    else:\n",
        "      features_data = features_scaler.fit_transform(df_train[[\"pixel\", \"tide\", \"flow\"]])\n",
        "      features_data_test = features_scaler.fit_transform(df_test[[\"pixel\", \"tide\", \"flow\"]])\n",
        "      #features_data = df_train[[\"pixel\", \"tide\", \"flow\"]].to_numpy()\n",
        "      #features_data_test = df_test[[\"pixel\", \"tide\", \"flow\"]].to_numpy()    \n",
        "    \n",
        "    target_data = target_scaler.fit_transform(df_train[[\"pixel\"]])\n",
        "    target_data_test = target_scaler.fit_transform(df_test[[\"pixel\"]])\n",
        "    #target_data = df_train[[\"pixel\"]].to_numpy()\n",
        "    #target_data_test = df_test[[\"pixel\"]].to_numpy()\n",
        "    \n",
        "    x_sampled, y_sampled = multi_data_prep(features_data, target_data)\n",
        "    x_test, y_test = multi_data_prep(features_data_test, target_data_test)\n",
        "\n",
        "  ### Reshape the data so that it can fit into the network.\n",
        "  x_sampled = x_sampled.reshape((x_sampled.shape[0], x_sampled.shape[1], n_features))\n",
        "  x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], n_features))\n",
        "  '''\n",
        "  print(\"\\nx, y size and shape\")\n",
        "  print(x_sampled.size, x_sampled.shape)\n",
        "  print(y_sampled.size, y_sampled.shape)\n",
        "  print(\"\\nVisualize the samples.\")\n",
        "  print(pixel_data_train[0:20])\n",
        "  ### summarize the data, sample pair. ([T-2, T-1]\tT)\n",
        "  for i in range(9):\n",
        "    print(x_sampled[i], y_sampled[i])\n",
        "  '''\n",
        "\n",
        "  ### Start training the model.\n",
        "  if (stateful):\n",
        "    model = build_model_stateful(n_features)\n",
        "    print(\"This is a \" + state_name +'_' + model_type +\" LSTM model mode \" + str(mode))\n",
        "    model.summary()\n",
        "    # Train network. Stateful LSTM model has to be train in this for loop manner. \n",
        "    for i in range(n_epoch):\n",
        "      # save the lowest loss model\n",
        "      checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True,save_weights_only=False, mode='min')\n",
        "      # Reduce learning rate for every 10 epochs if loss doesn't reduce. new_learning_rate = current_learning_rate * factor.\n",
        "      rrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, min_delta=0.0, mode='min', min_lr=0.00001)\n",
        "      # Stop training the model if loss doesn't reduce after 200 epochs.\n",
        "      es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200, min_delta=0.0)\n",
        "      callbacks_list = [checkpoint,rrp,es]\n",
        "      model.fit(x_sampled[0:int(m*0.8)], y_sampled[0:int(m*0.8)], epochs=1, batch_size=1, verbose=1, shuffle=False, callbacks=callbacks_list, validation_data=(x_sampled[int(m*0.8):int(m*1.0)],y_sampled[int(m*0.8):int(m*1.0)]))\n",
        "      model.reset_states()\n",
        "  else:\n",
        "    model = build_model_stateless(n_features)\n",
        "    print(\"This is a \" + state_name +'_' + model_type +\" LSTM model mode\" + str(mode))\n",
        "    model.summary()\n",
        "    # Train network. \n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True,save_weights_only=False, mode='min')\n",
        "    rrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1,min_delta=0.0, mode='min', min_lr=0.00001)\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200, min_delta=0.0)\n",
        "    callbacks_list = [checkpoint,rrp,es]\n",
        "    model.fit(x_sampled[0:int(m*0.8)], y_sampled[0:int(m*0.8)], epochs=n_epoch, batch_size=n_batch, verbose=1, shuffle=True, callbacks=callbacks_list, validation_data=(x_sampled[int(m*0.8):int(m*1.0)],y_sampled[int(m*0.8):int(m*1.0)]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset on point: 2 , 2\n",
            "m: 1023\n",
            "This is a stateful_multivariate LSTM model mode 3\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (1, 2, 64)                19200     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, 32)                   12416     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (1, 10)                   330       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, 1)                    11        \n",
            "=================================================================\n",
            "Total params: 31,957\n",
            "Trainable params: 31,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "818/818 [==============================] - 8s 6ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0197 - val_mean_squared_error: 0.0197\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0181 - val_mean_squared_error: 0.0181\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0150 - mean_squared_error: 0.0150 - val_loss: 0.0179 - val_mean_squared_error: 0.0179\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0146 - mean_squared_error: 0.0146 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0143 - mean_squared_error: 0.0143 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0140 - mean_squared_error: 0.0140 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0139 - mean_squared_error: 0.0139 - val_loss: 0.0176 - val_mean_squared_error: 0.0176\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0137 - mean_squared_error: 0.0137 - val_loss: 0.0176 - val_mean_squared_error: 0.0176\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0135 - mean_squared_error: 0.0135 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0134 - mean_squared_error: 0.0134 - val_loss: 0.0175 - val_mean_squared_error: 0.0175\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
            "818/818 [==============================] - 3s 4ms/step - loss: 0.0132 - mean_squared_error: 0.0132 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0131 - mean_squared_error: 0.0131 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0131 - mean_squared_error: 0.0131 - val_loss: 0.0183 - val_mean_squared_error: 0.0183\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0131 - mean_squared_error: 0.0131 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0176 - val_mean_squared_error: 0.0176\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0127 - mean_squared_error: 0.0127 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0175 - val_mean_squared_error: 0.0175\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0125 - mean_squared_error: 0.0125 - val_loss: 0.0175 - val_mean_squared_error: 0.0175\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0125 - mean_squared_error: 0.0125 - val_loss: 0.0175 - val_mean_squared_error: 0.0175\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0124 - mean_squared_error: 0.0124 - val_loss: 0.0182 - val_mean_squared_error: 0.0182\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.0176 - val_mean_squared_error: 0.0176\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.0185 - val_mean_squared_error: 0.0185\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0118 - mean_squared_error: 0.0118 - val_loss: 0.0181 - val_mean_squared_error: 0.0181\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0186 - val_mean_squared_error: 0.0186\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0125 - mean_squared_error: 0.0125 - val_loss: 0.0180 - val_mean_squared_error: 0.0180\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0115 - mean_squared_error: 0.0115 - val_loss: 0.0187 - val_mean_squared_error: 0.0187\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0115 - mean_squared_error: 0.0115 - val_loss: 0.0189 - val_mean_squared_error: 0.0189\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0179 - val_mean_squared_error: 0.0179\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0180 - val_mean_squared_error: 0.0180\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0112 - mean_squared_error: 0.0112 - val_loss: 0.0190 - val_mean_squared_error: 0.0190\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0185 - val_mean_squared_error: 0.0185\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0198 - val_mean_squared_error: 0.0198\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0197 - val_mean_squared_error: 0.0197\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0176 - val_mean_squared_error: 0.0176\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0189 - val_mean_squared_error: 0.0189\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0185 - val_mean_squared_error: 0.0185\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0110 - mean_squared_error: 0.0110 - val_loss: 0.0182 - val_mean_squared_error: 0.0182\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0107 - mean_squared_error: 0.0107 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0201 - val_mean_squared_error: 0.0201\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0201 - val_mean_squared_error: 0.0201\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0117 - mean_squared_error: 0.0117 - val_loss: 0.0206 - val_mean_squared_error: 0.0206\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0183 - val_mean_squared_error: 0.0183\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0103 - mean_squared_error: 0.0103 - val_loss: 0.0203 - val_mean_squared_error: 0.0203\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0101 - mean_squared_error: 0.0101 - val_loss: 0.0193 - val_mean_squared_error: 0.0193\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0212 - val_mean_squared_error: 0.0212\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0205 - val_mean_squared_error: 0.0205\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.0201 - val_mean_squared_error: 0.0201\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0101 - mean_squared_error: 0.0101 - val_loss: 0.0198 - val_mean_squared_error: 0.0198\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.0198 - val_mean_squared_error: 0.0198\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0227 - val_mean_squared_error: 0.0227\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0103 - mean_squared_error: 0.0103 - val_loss: 0.0208 - val_mean_squared_error: 0.0208\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0096 - mean_squared_error: 0.0096 - val_loss: 0.0222 - val_mean_squared_error: 0.0222\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0092 - mean_squared_error: 0.0092 - val_loss: 0.0202 - val_mean_squared_error: 0.0202\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0187 - val_mean_squared_error: 0.0187\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0255 - val_mean_squared_error: 0.0255\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0223 - val_mean_squared_error: 0.0223\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0096 - mean_squared_error: 0.0096 - val_loss: 0.0220 - val_mean_squared_error: 0.0220\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0210 - val_mean_squared_error: 0.0210\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0089 - mean_squared_error: 0.0089 - val_loss: 0.0223 - val_mean_squared_error: 0.0223\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0091 - mean_squared_error: 0.0091 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0089 - mean_squared_error: 0.0089 - val_loss: 0.0210 - val_mean_squared_error: 0.0210\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0115 - mean_squared_error: 0.0115 - val_loss: 0.0206 - val_mean_squared_error: 0.0206\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0092 - mean_squared_error: 0.0092 - val_loss: 0.0205 - val_mean_squared_error: 0.0205\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0089 - mean_squared_error: 0.0089 - val_loss: 0.0234 - val_mean_squared_error: 0.0234\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0249 - val_mean_squared_error: 0.0249\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0095 - mean_squared_error: 0.0095 - val_loss: 0.0217 - val_mean_squared_error: 0.0217\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0209 - val_mean_squared_error: 0.0209\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0210 - val_mean_squared_error: 0.0210\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0231 - val_mean_squared_error: 0.0231\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0231 - val_mean_squared_error: 0.0231\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0268 - val_mean_squared_error: 0.0268\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0219 - val_mean_squared_error: 0.0219\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0284 - val_mean_squared_error: 0.0284\n",
            "818/818 [==============================] - 4s 4ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0236 - val_mean_squared_error: 0.0236\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0221 - val_mean_squared_error: 0.0221\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0219 - val_mean_squared_error: 0.0219\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0092 - mean_squared_error: 0.0092 - val_loss: 0.0186 - val_mean_squared_error: 0.0186\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0218 - val_mean_squared_error: 0.0218\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0208 - val_mean_squared_error: 0.0208\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0232 - val_mean_squared_error: 0.0232\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0210 - val_mean_squared_error: 0.0210\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0215 - val_mean_squared_error: 0.0215\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0238 - val_mean_squared_error: 0.0238\n",
            "818/818 [==============================] - 4s 5ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0211 - val_mean_squared_error: 0.0211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VynCwWApsxwi",
        "outputId": "3c76050f-0f4c-48a9-fb41-6c5b1e8bcc0f"
      },
      "source": [
        "# Demonstrate prediction\n",
        "model = load_model(filepath)\n",
        "\n",
        "loss = model.evaluate(x_sampled[0:int(m*0.8)], y_sampled[0:int(m*0.8)], batch_size=1)\n",
        "print(\"\\nThe model loss on train is: \" + str(loss[0]))\n",
        "loss = model.evaluate(x_test,y_test, batch_size=1)\n",
        "print(\"\\nThe model loss on test is: \" + str(loss[0]))\n",
        "\n",
        "# for i in range(x_test.shape[0]):\n",
        "#   testX, testy = x_test[i], y_test[i]\n",
        "#   testX = testX.reshape((1, n_steps, n_features))\n",
        "#   yhat = model.predict(testX, batch_size=1)\n",
        "#   yhat = target_scaler.inverse_transform(yhat)\n",
        "#   print('>Expected=%.1f, Predicted=%.1f' % (testy, yhat))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "818/818 [==============================] - 2s 2ms/step - loss: 0.0079 - mean_squared_error: 0.0079\n",
            "\n",
            "The model loss on train is: 0.007877498865127563\n",
            "97/97 [==============================] - 0s 2ms/step - loss: 0.0217 - mean_squared_error: 0.0217\n",
            "\n",
            "The model loss on test is: 0.021680448204278946\n"
          ]
        }
      ]
    }
  ]
}