{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "constructing_fs_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "54tDmNBy4BJW",
        "pif2yauPnBfh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fookseng/LSTM/blob/main/constructing_fs_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6vJR2ubwSLq"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow # for image display\n",
        "from tensorflow.keras.models import Sequential, model_from_json, load_model\n",
        "from tensorflow.keras.layers import InputLayer, Dense, Activation, Flatten, LeakyReLU, ReLU, PReLU, Dropout, BatchNormalization, LSTM\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Nadam, Adadelta \n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from PIL import Image as im\n",
        "from keras.preprocessing.image import image\n",
        "import shutil\n",
        "import imageio\n",
        "from numpy import array\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaKYT2w7wnNJ",
        "outputId": "394b86a2-5ee7-4a33-eb5c-ae551be891dd"
      },
      "source": [
        "# Check tensorflow version\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYeAD6fCwvwO",
        "outputId": "f5f5dba5-06c9-471d-ab3d-41e79e592e35"
      },
      "source": [
        "# Mount Google Drive to access data\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54tDmNBy4BJW"
      },
      "source": [
        "# Variables initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5OY9OQC4FBA"
      },
      "source": [
        "# The data path of \"continuous_for_3\".\n",
        "path = '/content/gdrive/Shareddrives/namr - water quality forecast/1005data_OutletOnly(8x12)/CHL/continuous_for_3'\n",
        "# The path for saving model\n",
        "savePath = '/content/gdrive/Shareddrives/namr - water quality forecast/fs/result/'\n",
        "# Learning Rate\n",
        "learningRate = 0.001\n",
        "# Stateful or Stateless. If Stateful, set it to \"True\", else \"False\"\n",
        "stateful = False\n",
        "state_name = 'stateful' if stateful else 'stateless'\n",
        "# TSS or CHL model? Set the variable 'tss' to True if this is a TSS model, else false.\n",
        "tss = False\n",
        "model_name = \"tss\" if tss else 'chl'\n",
        "# The coordinate of pixel\n",
        "x_pos, y_pos = 5, 7\n",
        "# 存model路徑\n",
        "filepath = savePath +'/model_'+ state_name+ '_'+ str(x_pos)+ '_'+ str(y_pos)+ '_' +model_name+'.h5'\n",
        "# choose a number of time steps\n",
        "n_steps = 2\n",
        "# number of features\n",
        "n_features = 1\n",
        "# Batch size. For stateful LSTM, batch size must be set properly. Change the batch size only if you sure what you are doing.\n",
        "n_batch = 6\n",
        "# Number of epoch. Do not set too high, lower epoch is better for LSTM network.\n",
        "n_epoch = 100\n",
        "\n",
        "# Define some variables.\n",
        "pixel_value_T2 = []\n",
        "pixel_value_T1 = []\n",
        "pixel_value_T = []\n",
        "pattern_data = []\n",
        "flow_data = []\n",
        "tide_data = []\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pif2yauPnBfh"
      },
      "source": [
        "# Function Declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFEMxL5vy-6I"
      },
      "source": [
        "def load_data():\n",
        "  images_T2 = np.load(path +'/image_array_0.npy')\n",
        "  images_T1 = np.load(path +'/image_array_1.npy')\n",
        "  images_T = np.load(path +'/image_array_2.npy')\n",
        "  flow_T2 = np.load(path +'/flow_array_0.npy')\n",
        "  flow_T1 = np.load(path +'/flow_array_1.npy')\n",
        "  flow_T = np.load(path +'/flow_array_2.npy')\n",
        "  tide_T2 = np.load(path +'/delta_tide_array_0.npy')\n",
        "  tide_T1 = np.load(path +'/delta_tide_array_1.npy')\n",
        "  tide_T = np.load(path +'/delta_tide_array_2.npy')\n",
        "\n",
        "  print(images_T2.size, images_T2.shape)\n",
        "  print(images_T1.size, images_T1.shape)\n",
        "  print(images_T.size, images_T.shape)\n",
        "  print(flow_T2.size, flow_T2.shape)\n",
        "  print(flow_T1.size, flow_T1.shape)\n",
        "  print(flow_T.size, flow_T.shape)\n",
        "  print(tide_T2.size, tide_T2.shape)\n",
        "  print(tide_T1.size, tide_T1.shape)\n",
        "  print(tide_T.size, tide_T.shape)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QHCz_Rv1oH8"
      },
      "source": [
        "def createData(x, y):\n",
        "    print(\"Creating dataset on point:\", x,\",\", y)\n",
        "    for i in range(tide_T.size):\n",
        "        pixel_value_T2.append(images_T2[i][x][y])\n",
        "        pixel_value_T1.append(images_T1[i][x][y])\n",
        "        pixel_value_T.append(images_T[i][x][y])\n",
        "        pattern_data.append(images_T2[i][x][y])\n",
        "        pattern_data.append(images_T1[i][x][y])\n",
        "        pattern_data.append(images_T[i][x][y])\n",
        "        flow_data.append(flow_T2[i][0])\n",
        "        flow_data.append(flow_T1[i][0])\n",
        "        flow_data.append(flow_T[i][0])\n",
        "        tide_data.append(tide_T2[i][0])\n",
        "        tide_data.append(tide_T1[i][0])\n",
        "        tide_data.append(tide_T[i][0])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "522LMIc9YS0U"
      },
      "source": [
        "# design network\n",
        "def build_model_stateful(n_features):\n",
        "    model = Sequential()\n",
        "    #model.add(LSTM(64, input_shape=(n_steps, n_features), return_sequences= True, batch_input_shape=(n_batch, x_sampled.shape[1], x_sampled.shape[2]), stateful=True))\n",
        "    model.add(LSTM(64, input_shape=(n_steps, n_features), return_sequences= True, batch_input_shape=(1, x_sampled.shape[1], x_sampled.shape[2]), stateful=True))\n",
        "    model.add(LSTM(32, input_shape=(n_steps, n_features), return_sequences=False, batch_input_shape=(1, x_sampled.shape[1], x_sampled.shape[2]), stateful=True))\n",
        "    model.add(Dense(10, activation=ReLU()))\n",
        "    model.add(Dense(1, activation=ReLU()))\n",
        "    model.compile(loss='mean_squared_error', optimizer= Nadam(learning_rate=learningRate), metrics=['mean_squared_error'])\n",
        "    return model\n",
        "def build_model_stateless(n_features):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=(n_steps, n_features), return_sequences= True))\n",
        "    model.add(LSTM(32, return_sequences=False))\n",
        "    model.add(Dense(10, activation=ReLU()))\n",
        "    model.add(Dense(1, activation=ReLU()))\n",
        "    model.compile(loss='mean_squared_error', optimizer= Nadam(learning_rate=learningRate), metrics=['mean_squared_error'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OQIBg_PrhMD"
      },
      "source": [
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(0, len(sequence), 3):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGSPWfkepubc"
      },
      "source": [
        "# **Main **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuFR9pQk4-mc"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  load_data()\n",
        "  # Total number of data\n",
        "  m = tide_T.size\n",
        "  print(\"m: \" + str(m))\n",
        "  \n",
        "  # Create data and save as pandas DataFrame\n",
        "  createData(x_pos, y_pos)\n",
        "  dic = {'pixel': pattern_data, 'flow': flow_data, 'tide': tide_data}\n",
        "  df = pd.DataFrame(dic)\n",
        "  \n",
        "  # Observe statistics and frequency of dataset.\n",
        "  plot_cols = ['pixel', 'flow', 'tide']\n",
        "  plot_features = df[plot_cols]\n",
        "  _ = plot_features.plot(subplots=True)\n",
        "  plot_features = df[plot_cols][:80]\n",
        "  _ = plot_features.plot(subplots=True)\n",
        "  df.describe().transpose()\n",
        "  \n",
        "  # Split the data (7:2:1 == 784, 224, 112)\n",
        "  column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "  n = len(df)\n",
        "  train_df = df[0:int(n*0.7)]\n",
        "  val_df = df[int(n*0.7):int(n*0.9)]\n",
        "  test_df = df[int(n*0.9):]\n",
        "  num_features = df.shape[1]\n",
        "  print(\"\\nDataframe after splited into 7:2:1\")\n",
        "  print(train_df)\n",
        "  print(val_df)\n",
        "  print(test_df)\n",
        "\n",
        "  # split into samples. These samples will be fed into network for training.\n",
        "  x_sampled, y_sampled = split_sequence(pattern_data, n_steps)\n",
        "  # Reshape the data so that it can fit into the network.\n",
        "  x_sampled = x_sampled.reshape((x_sampled.shape[0], x_sampled.shape[1], n_features))\n",
        "  print(\"\\nx, y size and shape\")\n",
        "  print(x_sampled.size, x_sampled.shape)\n",
        "  print(y_sampled.size, y_sampled.shape)\n",
        "  print(\"\\nVisualize the samples.\")\n",
        "  print(pattern_data[0:20])\n",
        "  # summarize the data, sample pair. ([T-2, T-1]\tT)\n",
        "  for i in range(9):\n",
        "    print(x_sampled[i], y_sampled[i])\n",
        "\n",
        "  # if (stateful):\n",
        "  #   model = build_model_stateful(n_features)\n",
        "  #   print(\"This is a stateful LSTM model\")\n",
        "  #   model.summary()\n",
        "    \n",
        "  #   # Train network. Stateful LSTM model have to be train in this for loop manner. \n",
        "  #   for i in range(n_epoch):\n",
        "  #     # 保留最低loss\n",
        "  #     checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True,save_weights_only=False, mode='min')\n",
        "  #     # 每過100回合loss都沒降的話就把learning rate乘上factor，最多降到0.00001\n",
        "  #     rrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100, verbose=1,min_delta=0.0, mode='min', min_lr=0.00001)\n",
        "  #     # 若經過400回合，loss都沒有降低的話就提早停止訓練\n",
        "  #     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=400, min_delta=0.0)\n",
        "  #     callbacks_list = [checkpoint,rrp,es]\n",
        "  #     model.fit(x_sampled[0:int(m*0.7)], y_sampled[0:int(m*0.7)], epochs=1, batch_size=1, verbose=1, shuffle=False, callbacks=callbacks_list, validation_data=(x_sampled[int(m*0.7):int(m*0.9)],y_sampled[int(m*0.7):int(m*0.9)]))\n",
        "  #     model.reset_states()\n",
        "  # else:\n",
        "  #   model = build_model_stateless(n_features)\n",
        "  #   print(\"This is a stateless LSTM model\")\n",
        "  #   model.summary()\n",
        "  #   # Train network. \n",
        "  #   checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True,save_weights_only=False, mode='min')\n",
        "  #   rrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100, verbose=1,min_delta=0.0, mode='min', min_lr=0.00001)\n",
        "  #   es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=400, min_delta=0.0)\n",
        "  #   callbacks_list = [checkpoint,rrp,es]\n",
        "  #   model.fit(x_sampled[0:int(m*0.7)], y_sampled[0:int(m*0.7)], epochs=n_epoch, batch_size=n_batch, verbose=1, shuffle=True, callbacks=callbacks_list, validation_data=(x_sampled[int(m*0.7):int(m*0.9)],y_sampled[int(m*0.7):int(m*0.9)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CMhkKJc_FXC"
      },
      "source": [
        "# Demonstrate prediction\n",
        "model = load_model(filepath)\n",
        "x_test = x_sampled[int(m*0.9):]\n",
        "y_test = y_sampled[int(m*0.9):]\n",
        "\n",
        "for i in range(50):\n",
        "  testX, testy = x_test[i], y_test[i]\n",
        "  testX = testX.reshape((1, n_steps, n_features))\n",
        "  yhat = model.predict(testX, batch_size=1)\n",
        "  print('>Expected=%.1f, Predicted=%.1f' % (testy, yhat))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}